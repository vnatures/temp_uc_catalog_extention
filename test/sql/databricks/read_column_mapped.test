# name: test/sql/databricks/read_column_mapped.test
# description: test
# group: [databricks]

require parquet

require unity_catalog

require delta

require httpfs

require-env DATABRICKS_TOKEN

require-env DATABRICKS_ENDPOINT

require-env DATABRICKS_REGION

# Do not ignore 'HTTP' error messages!
set ignore_error_messages

statement ok
CREATE SECRET (
    TYPE UNITY_CATALOG,
    TOKEN '${DATABRICKS_TOKEN}',
    ENDPOINT '${DATABRICKS_ENDPOINT}',
    AWS_REGION '${DATABRICKS_REGION}'
);

# Attach to databricks through the unity catalog API
statement ok
ATTACH 'duckdblabs_testing' (TYPE UNITY_CATALOG, DEFAULT_SCHEMA 'main');

# Now test with column mapped table
query II
from duckdblabs_testing.main.evolution_simple_name_mode ORDER BY all
----
1	NULL
2	2

query II
from duckdblabs_testing.main.evolution_column_change ORDER BY a
----
value1	NULL
value3	NULL
value4	5

query II
SELECT a,b from duckdblabs_testing.main.evolution_column_change ORDER BY a
----
value1	NULL
value3	NULL
value4	5

# FIXME: virtual/rowid columns can't be delegated right now
mode skip

# Ensure we play ball with projections and generated columns, etc
query IIII
SELECT file_row_number, filename.substring(-8,8), b, a from duckdblabs_testing.main.evolution_column_change order by a
----
0	.parquet	NULL	value1
0	.parquet	NULL	value3
0	.parquet	5	value4
